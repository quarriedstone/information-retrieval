{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Nearest Neighbours Search\n",
    "\n",
    "Sometimes, when we are processing a user query, it may be **acceptable to retrieve a \"good guess\"** of \n",
    "nearest neighbours to the query **instead of true nearest neighbours**. In those cases, one can use an algorithm which doesn't guarantee to return the actual nearest neighbour in every case, **in return for improved speed or memory savings**. Thus, with the help of such algorithms one can do a **fast approximate search in a very large dataset**. Today we will expore two approaches based on graphs and trees.\n",
    "\n",
    "This is what we are going to do in this lab: \n",
    "\n",
    "1. Build a *navigable* small-world graph;\n",
    "2. Build a k-d tree;\n",
    "3. Try `Annoy`. \n",
    "\n",
    "## 0. Dataset preparation\n",
    "We will utilize [dataset with curious facts](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt). Using trained `doc2vec` [model](https://github.com/jhlau/doc2vec) (Associated Press News DBOW (0.6GB), we will infer vectors for every fact and normalize them.\n",
    "\n",
    "### 0.1. Loading doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# unpack a model into 3 files and target the main one\n",
    "# doc2vec.bin  <---------- this\n",
    "# doc2vec.bin.syn0.npy\n",
    "# doc2vec.bin.sin1neg.npy\n",
    "model = Doc2Vec.load('doc2vec.bin', mmap=None)\n",
    "print(type(model))\n",
    "print(type(model.infer_vector([\"to\", \"be\", \"or\", \"not\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data_url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "file_name= \"facts.txt\"\n",
    "urllib.request.urlretrieve(data_url, file_name)\n",
    "\n",
    "facts = []\n",
    "with open(file_name) as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        facts.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Transforming sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence.lower())\n",
    "\n",
    "def get_words_from_sentence(sentences):\n",
    "    for sentence in sentences: \n",
    "        yield word_tokenize(sentence.split('.', 1)[1])\n",
    "\n",
    "sent_vecs = np.array([])\n",
    "sent_vecs = np.array(list(model.infer_vector(words) for words in get_words_from_sentence(facts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Norming vectors (not graded)\n",
    "\n",
    "Complete the method of vector norming, you can copy your code from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vectors(A):\n",
    "    An = A.copy()\n",
    "    ### YOUR CODE HERE :)\n",
    "    ### NOT GRADED\n",
    "    return An\n",
    "\n",
    "def find_k_closest(query, dataset, k=5):    \n",
    "    index = list((i, v, np.dot(query, v)) for i, v in enumerate(dataset))    \n",
    "    return sorted(index, key=lambda pair: pair[2], reverse=True)[:k]\n",
    "\n",
    "sent_vecs_normed = norm_vectors(sent_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing Navigable Small World graph search\n",
    "\n",
    "Now you need to implement an efficient search procedure which would utilize small world properties. Starting from the random node, at each step you should move towards the closest node (in terms of **cosine simiarity**, in our case), meanwhile keeping and refreshing top-K nearest neighbours collection. \n",
    "\n",
    "### 1.1. Complete NSW index construction stage for this implementation\n",
    "\n",
    "You can refer to the `Nearest_Neighbor_Insert` algorithm which pseudocode is given in section 5 (p. 65) of the [original paper](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sortedcontainers\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm as vector_norm\n",
    "import pickle\n",
    "\n",
    "class Node:\n",
    "    ''' Graph node class. Major properties are `value` to access embedding and `neighbourhood` for adjacent nodes '''\n",
    "    def __init__(self, value, idx, label):\n",
    "        self.value = value\n",
    "        self.label = label\n",
    "        self.idx = idx\n",
    "        self.neighbourhood = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"`#{self.idx}: '{self.label} ~ {self.value}' ~ [{self.neighbourhood}]`\"\n",
    "        \n",
    "class NSWGraph:    \n",
    "    ## NB: by default constructor uses euclidean distance\n",
    "    ## Next block asks you to implement cosine distance\n",
    "    def __init__(self, values=None, dist=None):\n",
    "        '''Values are tuples or lists of (vector, label)'''\n",
    "        self.dist = dist if dist else self.eucl_dist\n",
    "        self.nodes = [Node(node[0], i, node[1]) for i, node in enumerate(values)] if values else []\n",
    "\n",
    "    def eucl_dist(self, v1, v2):\n",
    "        return distance.euclidean(v1, v2)\n",
    "          \n",
    "    def search_nsw_basic(self, query, visitedSet, candidates, result, top=5, guard_hops=100, callback=None):\n",
    "        ''' basic algorithm, takes vector query and returns a pair (nearest_neighbours, hops)'''\n",
    "        # taking random node as an entry point\n",
    "        tmpResult = sortedcontainers.SortedList()\n",
    "        entry = random.randint(0, len(self.nodes) - 1)\n",
    "        if entry not in visitedSet:\n",
    "            candidates.add((self.dist(query, self.nodes[entry].value), entry))\n",
    "        tmpResult.add((self.dist(query, self.nodes[entry].value), entry))\n",
    "        \n",
    "        hops = 0\n",
    "        while hops < guard_hops:\n",
    "            hops += 1\n",
    "            if len(candidates) == 0: break\n",
    "            \n",
    "            # 6 get element c closest from candidates (see paper 4.2.)\n",
    "            # 7 remove c from candidates\n",
    "            closest_dist, сlosest_id = candidates.pop(0)\n",
    "            \n",
    "            # k-th best of global result\n",
    "            # new stop condition from paper\n",
    "            # if c is further than k-th element from result\n",
    "            # than break repeat\n",
    "            #! NB this statemrnt from paper will not allow to converge in first run.\n",
    "            #! thus we use tmpResult if result is empty\n",
    "            if len(result or tmpResult) >= top:\n",
    "                if (result or tmpResult)[top-1][0] < closest_dist: break\n",
    "\n",
    "            #  for every element e from friends of c do:\n",
    "            for e in self.nodes[сlosest_id].neighbourhood:\n",
    "                # 13 if e is not in visitedSet than\n",
    "                if e not in visitedSet:                   \n",
    "                    d = self.dist(query, self.nodes[e].value)\n",
    "                    # 14 add e to visitedSet, candidates, tempRes\n",
    "                    visitedSet.add(e)\n",
    "                    candidates.add((d, e))\n",
    "                    tmpResult.add((d, e))\n",
    "                    \n",
    "            if callback is not None:\n",
    "                callback(self.nodes[сlosest_id].value, tmpResult)\n",
    "\n",
    "        return tmpResult, hops\n",
    "        \n",
    "    def multi_search(self, query, attempts=1, top=5):   \n",
    "        '''Implementation of `K-NNSearch`, but without keeping the visitedSet'''\n",
    "\n",
    "        # share visitedSet among searched. Paper, 4.2.p2\n",
    "        visitedSet, candidates, result = set(), sortedcontainers.SortedList(), sortedcontainers.SortedList()\n",
    "        \n",
    "        for i in range(attempts):\n",
    "            closest, hops = self.search_nsw_basic(query, visitedSet, candidates, result, top=top)\n",
    "            result.update(closest)\n",
    "            result = sortedcontainers.SortedList(set(result))\n",
    "            \n",
    "        return [v for k, v in result[:top]]\n",
    "    \n",
    "    ####################################\n",
    "    ##   COMPLETE THIS METHOD         ##\n",
    "    ####################################\n",
    "    def build_navigable_graph(self, values, attempts=3, verbose=False, M=None):\n",
    "        '''Accepts container with values. Returns list with graph nodes'''\n",
    "        # create graph with one node\n",
    "        self.nodes.append(Node(values[0][0], len(self.nodes), values[0][1]))\n",
    "        \n",
    "        # The tests indicate [36] that at least for Euclid data with\n",
    "        # d = 1...20, the optimal value for number of neighbors to\n",
    "        # connect (f) is about 3d\n",
    "        d = len(values[0][0])\n",
    "        f = 3 * d if M is None else M\n",
    "        if verbose:\n",
    "            print(f\"Data dimensionality detected is {d}. regularity = {f}\")\n",
    "        \n",
    "        ##################################\n",
    "        ### TODO implement this part:                                    #\n",
    "        ### 1. Repeat for each node                                      #\n",
    "        ### 2. Search for f neighbours with `attempts` value             #\n",
    "        ### 3. Create a node, insert in a list and connect it to obtained#\n",
    "        ### Use node.neighbourhood property to store neighbour indices   #\n",
    "        ##################################\n",
    "        \n",
    "        for i in range(1, len(values)):\n",
    "            val = values[i][0]\n",
    "            \n",
    "            ## SOME CODE IS MISSING HERE\n",
    "            \n",
    "            if verbose:\n",
    "                if i * 10 % len(values) == 0:\n",
    "                    print(f\"\\t{100 * i / len(values):.2f}% of graph construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Implement cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    ## implement cosine-based metric for a pair of vectors (which can be just lists)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Test your search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensionality detected is 2. regularity = 6\n",
      "\t50.00% of graph construction\n",
      "Searching for {q}:\n",
      "0 `#0: '[0, 0]' ~ [{1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13}]`\n",
      "1 `#1: '[0, 0]' ~ [{0, 2, 3, 4, 5, 6, 8, 9, 12}]`\n",
      "4 `#4: '[1, 0]' ~ [{0, 1, 2, 3, 5, 6, 7, 8, 9, 12, 13}]`\n",
      "5 `#5: '[1, 0]' ~ [{0, 1, 2, 3, 4, 6, 7, 8, 9, 12, 13}]`\n",
      "8 `#8: '[2, 0]' ~ [{0, 1, 3, 4, 5, 7, 9, 10, 11, 12, 13}]`\n"
     ]
    }
   ],
   "source": [
    "g = NSWGraph(dist=cosine)\n",
    "data = [([0, 0], \"a\"), ([0, 0], \"a\"), ([100, 101], \"b\"), ([100, 100], \"b\")]\n",
    "data += [([1, 0], \"a\"), ([1, 0], \"a\"), ([101, 101], \"b\"), ([101, 100], \"b\")]\n",
    "data += [([2, 0], \"a\"), ([2, 0], \"a\"), ([102, 101], \"b\"), ([102, 100], \"b\")]\n",
    "data += [([3, 0], \"a\"), ([3, 0], \"a\"), ([103, 101], \"b\"), ([104, 100], \"b\")]\n",
    "g.build_navigable_graph(data, verbose=True)\n",
    "\n",
    "q = [0, 0]\n",
    "print(f\"Searching for {q}:\")\n",
    "\n",
    "for idx in g.multi_search([0, 0]):\n",
    "    print(idx, g.nodes[idx])\n",
    "    assert g.nodes[idx].value[0] < 10 and g.nodes[idx].value[1] < 10, f\"{g.nodes[idx]} is not close to {q}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. [Bonus] Visualize the graph\n",
    "\n",
    "Draw a picture with NSW graph with 100 2D nodes distributed uniformly in `[0, 1] x [0, 1]` range. Use `matplotlib` or `seaborn` or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement KD-tree\n",
    "\n",
    "Within this cell you will iteratively implement major methods of [kd-tree](https://en.wikipedia.org/wiki/K-d_tree) data structure. Biggest part of code is already written - just fill remaining gaps. \n",
    "\n",
    "This particular implementation is designed to be **non-homogeneous**, which means it **stores values only in leaf nodes**. But this doesn't limit you if you want to implement another approach.\n",
    "\n",
    "Each leaf node can hold up to `leaf_capacity` items.\n",
    "\n",
    "Item stored in an index is a tuple: `(coordinate_k_dim_vector, value)`.\n",
    "\n",
    "Search requires only `coordinate_k_dim_vector` and returns relevant item(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([0.61578687, 0.46977607, 0.76302562]), 'stub value 0'), (array([0.10835509, 0.80657256, 0.07666364]), 'stub value 1'), (array([0.7398146 , 0.29638078, 0.32806704]), 'stub value 2')]\n",
      "500000 rows generated in 0.64 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K_random = 3\n",
    "maxsize = 500000\n",
    "start = time.time()\n",
    "R = np.random.rand(maxsize, K_random)\n",
    "R = [(row, \"stub value {}\".format(i)) for i, row in enumerate(R)]\n",
    "print(R[:3])\n",
    "finish = time.time()\n",
    "print(\"{} rows generated in {:.2f} s\".format(len(R), finish - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Build procedure ###\n",
    "`build_kd_tree()` method implements the core idea. This is the suggested idea, but you can modify it slightly:\n",
    "- tree is non-homogenous, thus, items are stored in leaf nodes only.\n",
    "- `self.pivot` should be assigned the median value for correspoinding `depth % K` coordinate.\n",
    "- `left` variable should hold all the items <= than `self.pivot`.\n",
    "- `right` variable should hold all the items > than `self.pivot`.\n",
    "- `node_capacity=X` allows to store up to `X` items in a leaf node.\n",
    "\n",
    "This is how it works for `K=2, node_capacity=2`:\n",
    "![kd-tree](http://sprotasov.ru/files/kd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Complete the following methods (EXPENSIVE)\n",
    "- `build_kd_tree`, \n",
    "- `kd_find_leaf`,\n",
    "- `kd_insert_with_split`,\n",
    "- `get_nn`,\n",
    "- `get_in_range` (optional, to do real data test)\n",
    "\n",
    "Test for each of them will come in following blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, K=None, parent=None):\n",
    "        assert K is not None or parent, \"Either `K` should be provided for root node, or `parent` for internal nodes\"\n",
    "        # Reference to parent node. Used in ANNS search\n",
    "        self.parent = parent\n",
    "        # depth start from 0. To compute dimension, relevant to the level, use (self.depth % self.K)\n",
    "        self.depth = (parent.depth + 1) if parent else 0\n",
    "        # K means number of vector dimensions\n",
    "        self.K = parent.K if parent else K\n",
    "        # value, which splits subspace into to parts using hyperplane: item[self.depth % self.K] == self.pivot\n",
    "        # pivot is empty for any leaf node.\n",
    "        self.pivot = None\n",
    "        # left and right child nodes\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # collection of items\n",
    "        self.items = None\n",
    "        \n",
    "    def build_kd_tree(self, items, leaf_capacity=4):\n",
    "        '''Takes a list of items and arranges it in a kd-tree'''\n",
    "        assert items is not None, \"Please provide at least one point\"\n",
    "        # put all items in the node if they fit into limit\n",
    "        if len(items) <= leaf_capacity:\n",
    "            self.items = items\n",
    "        # or else split items into 2 subnodes using median (?!) value\n",
    "        else:\n",
    "            self.items = None\n",
    "            self.left = Node(parent=self)\n",
    "            self.right = Node(parent=self)\n",
    "            \n",
    "            #TODO 1.A.: fill in the code to initialize internal node.\n",
    "            # Be careful: there may be multiple items with the same values as pivot,\n",
    "            # make sure they go to the same child.\n",
    "            # Also, there may be duplicate items, and you need to deal with them\n",
    "            self.pivot = None     # here you should write median value with respect to coordinate\n",
    "            left = None           # those items, which are smaller than the pivot value\n",
    "            right = None          # those items, which are greater than the pivot value\n",
    "            \n",
    "            \n",
    "            self.left.build_kd_tree(left)\n",
    "            self.right.build_kd_tree(right)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def kd_find_leaf(self, key):\n",
    "        ''' returns a node where key should be stored (but can be not present)'''\n",
    "        if self.pivot is None or self.items is not None: # leaf node OR empty root\n",
    "            return self\n",
    "        else:\n",
    "            \n",
    "            #TODO 1.B. This is a basic operation for travesing the tree.\n",
    "            # define correct path to continue recursion\n",
    "            \n",
    "            return None\n",
    "            \n",
    "#     def kd_insert_no_split(self, item):\n",
    "#         '''Naive implementation of insert into leaf node. It is not used in tests of this tutorial.'''\n",
    "#         node = self.kd_find_leaf(item[0])\n",
    "#         node.items.append(item)\n",
    "        \n",
    "    def kd_insert_with_split(self, item, leaf_capacity=4):\n",
    "        '''This method recursively splits the nodes into 2 child nodes if they overflow `leaf_capacity`'''\n",
    "        \n",
    "        #TODO 1.C. This is very simple insertion procedure.\n",
    "        # Split the node if it cannot accept one more item.\n",
    "        # HINT: reuse kd_find_leaf() and build_kd_tree() methods if possible\n",
    "        \n",
    "        pass\n",
    "        \n",
    "            \n",
    "    def get_nn(self, key, knn):\n",
    "        '''Return K approximate nearest neighbours for a given key'''\n",
    "        node = self.kd_find_leaf(key)\n",
    "        best = []\n",
    "        \n",
    "        #TODO 1.D. ANN search.\n",
    "        # write here the code which returns `knn` \n",
    "        # approximate nearest neighbours with respect to euclidean distance\n",
    "        # basically, you need to move up through the parents chain until the number of elements\n",
    "        # in a parent subtree is more or equal too the expected number of nearest neighbors,\n",
    "        # and then return top-k elements of this subtree sorted by euclidean distance\n",
    "        # HINT: you can use [scipy.spatial.]distance.euclidean(a, c) - it is already imported\n",
    "        \n",
    "                \n",
    "        return best[:knn]\n",
    "    \n",
    "    def get_in_range(self, lower_left_bound_key, upper_right_bound_key):\n",
    "        '''Runs range query. Returns all items bounded by the given corners: `lower_left_bound_key`, `upper_right_bound_key`'''\n",
    "        result = []\n",
    "        if self.pivot is None or self.items is not None: # internal node OR empty root\n",
    "            #TODO 3.B.: This is a leaf node. Select only those items from self.item\n",
    "            # which fall into a given range\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            #TODO 3.B.: This is an internal node.\n",
    "            # write recursive code to collect corresponding data from subtrees\n",
    "            # compare pivot to the bounds to decide whether to consider any of its children for search\n",
    "            \n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Test that building time is growing linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10, 20, ..., 100% of R\n",
    "step = len(R) // 10\n",
    "sizes, build_times = [], []\n",
    "\n",
    "# tqdm_notebook draws a progress bar. If it doesnt't work in your environment, just remove it.\n",
    "for size in tqdm_notebook(range(1, len(R) + 1, step)):\n",
    "    sample = R[:size]\n",
    "    start = time.time()\n",
    "    kdtree = Node(K=K_random).build_kd_tree(sample)\n",
    "    finish = time.time()\n",
    "    sizes.append(size)\n",
    "    build_times.append((finish - start) * 1000)\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "a = plt.axes()\n",
    "a.set_xlabel('dataset size')\n",
    "a.set_ylabel('time, ms')\n",
    "plt.plot(sizes, build_times)\n",
    "plt.title(\"KD-tree build speed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Test that search time is fast and of a millisecond scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = len(R) // 10\n",
    "sizes, search_times = [], []\n",
    "\n",
    "for size in tqdm_notebook(range(step, len(R) + 1, step)):\n",
    "    kdtree = Node(K=K_random).build_kd_tree(R[:size])\n",
    "    \n",
    "    iterations = 100\n",
    "    duration = None\n",
    "    for i in range(iterations):\n",
    "        x = R[random.randint(0, len(R) - 1)]\n",
    "        start = time.time()\n",
    "        kdtree.kd_find_leaf(x[0])\n",
    "        finish = time.time()\n",
    "        duration = max(duration, finish - start) if duration else finish - start\n",
    "        \n",
    "    sizes.append(size)\n",
    "    search_times.append(duration * 1000)\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "a = plt.axes()\n",
    "a.set_xlabel('dataset size')\n",
    "a.set_ylabel('worst search time (100 attempts), ms')\n",
    "plt.plot(sizes, search_times)\n",
    "plt.title(\"KD-tree search speed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Insertion ###\n",
    "Inserting speed should be growing similar to `log` function with respect to data size.\n",
    "\n",
    "Now let's see how fast the tree depth is growing for different capacity set-ups. Implement the remaining part of `kd_insert_with_split()` and run the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5000\n",
    "sample_size = 500000\n",
    "trees = [\n",
    "            Node(K=K_random), \n",
    "            Node(K=K_random), \n",
    "            Node(K=K_random)\n",
    "]\n",
    "capacities = [1, 4, 10]\n",
    "\n",
    "\n",
    "sizes, depths = [], [[], [], []]\n",
    "max_depth = [0, 0, 0]\n",
    "for i in tqdm_notebook(range(0, sample_size, step)):\n",
    "    for item in R[i:i+step]:\n",
    "        for t in range(3):\n",
    "            trees[t].kd_insert_with_split((item[0], \"stub value test 1.C.\"), leaf_capacity=capacities[t])\n",
    "            max_depth[t] = max(trees[t].kd_find_leaf(item[0]).depth, max_depth[t])\n",
    "    sizes.append(i)\n",
    "    for t in range(3):\n",
    "        depths[t].append(max_depth[t])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "a = plt.axes()\n",
    "a.set_xlabel('iteration of inserting 5000 points')\n",
    "a.set_ylabel('tree depth after insertions')\n",
    "plt.plot(sizes, depths[0], label=\"capacity={}\".format(capacities[0]))\n",
    "plt.plot(sizes, depths[1], label=\"capacity={}\".format(capacities[1]))\n",
    "plt.plot(sizes, depths[2], label=\"capacity={}\".format(capacities[2]))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"KD-tree insertion speed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. ANNS search ### \n",
    "This block tests `get_nn()` method, which performs approximate nearest neighbour search. Please, complete this method. After you run this block you will see how NN count influences search speed. It should be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN SEARCH\n",
    "kdtree = Node(K=K_random).build_kd_tree(R)\n",
    "times = []\n",
    "nns = [10, 100, 1000, 10000, 20000, 40000]\n",
    "iterations = 10\n",
    "for nn in tqdm_notebook(nns):\n",
    "    start = time.time()\n",
    "    for t in range(iterations):\n",
    "        x = R[random.randint(0, len(R) - 1)]\n",
    "        kdtree.get_nn(x[0], nn)\n",
    "    finish = time.time()\n",
    "    times.append((finish - start) * 1000 / iterations)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "a = plt.axes()\n",
    "a.set_xlabel('How many NNs we request')\n",
    "a.set_ylabel('time of search, ms')\n",
    "plt.title(\"KD-tree depth growth with insertions\")\n",
    "plt.plot(nns,times)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Real data (self studying)\n",
    "\n",
    "Please refer to [this notebook](08%20Approximate%20NN%20Search%20trees%20template.ipynb#3.-Real-data) to see how the methods work with real data on a map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus with [Annoy](https://github.com/spotify/annoy) library ## \n",
    "\n",
    "Now you have your own index data structure. How far is it from industrial implementations?\n",
    "\n",
    "First of all, let's install and import the requirement. Just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user annoy\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Build the index upon the same data using 10 trees, cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = AnnoyIndex(...)\n",
    "## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a test for KNN search. \n",
    "\n",
    "Is it much faster?\n",
    "\n",
    "Is it assimpotically faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN SEARCH\n",
    "times_annoy = []\n",
    "nns = [10, 100, 1000, 10000, 20000, 40000]\n",
    "iterations = 10\n",
    "for nn in tqdm_notebook(nns):\n",
    "    start = time.time()\n",
    "    for t in range(iterations):\n",
    "        x = R[random.randint(0, len(R) - 1)]\n",
    "        index.get_nns_by_vector(x[0], nn)\n",
    "    finish = time.time()\n",
    "    times_annoy.append((finish - start) * 1000 / iterations)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "a = plt.axes()\n",
    "a.set_xlabel('How many NNs we request')\n",
    "a.set_ylabel('time, ms')\n",
    "plt.plot(nns, times_annoy, label='Annoy')\n",
    "# you can also build times array on the same graph\n",
    "plt.plot(nns, times, label='KD-tree')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"KD-tree VS Annoy. NN search speed\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
