{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Authentication in a service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. What do you store in your Google Drive?\n",
    "\n",
    "Sometimes it can be quite troublesome to crawl web data - for example, when you can't just collect data from web-pages because the authentification to a website is required. Today's tutorial is about a dataset of special type - namely, Google Drive data. You will need to get access to the system using OAuth protocol, download and parse files of different types.\n",
    "\n",
    "Plan. \n",
    "1. Download [this little archive](https://drive.google.com/open?id=1Xji4A_dEAm_ycnO0Eq6vxj7ThcqZyJZR), **unzip** it and place the folder anywhere inside your Google Drive. You should get a subtree of 6 folders with files of different types: presentations, pdf-files, texts, and even code.\n",
    "2. Go to [Google Drive API](https://developers.google.com/drive/api/v3/quickstart/python) documentation, read [intro](https://developers.google.com/drive/api/v3/about-sdk) and learn how to [search for files](https://developers.google.com/drive/api/v3/reference/files/list) and [download](https://developers.google.com/drive/api/v3/manage-downloads) them. Pay attention, that  working at `localhost` (jupyter) and at `google colab` can be slighty different. We expect you to run from localhost.\n",
    "3. Learn how to open from python such files as [pptx](https://python-pptx.readthedocs.io/en/latest/user/quickstart.html), pdf, docx or even use generalized libraries like [textract](https://textract.readthedocs.io/en/stable/index.html), save internal text in a file near.\n",
    "4. Write a code with returns names (with paths) of files for a given substring. Test on these queries.\n",
    "```\n",
    "segmentation\n",
    "algorithm\n",
    "classifer\n",
    "printf\n",
    "predecessor\n",
    "Шеннон\n",
    "Huffman\n",
    "function\n",
    "constructor\n",
    "machine learning\n",
    "dataset\n",
    "Протасов\n",
    "Protasov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Access GDrive ###\n",
    "\n",
    "Below is the example of how you can oranize your code - it's fine if you change it.\n",
    "\n",
    "Let's extract the list of all files that are contained (recursively) in t\n",
    "he folder of interest. In my case, I called it `air_oauth_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-api-python-client in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (1.12.8)\n",
      "Requirement already up-to-date: google-auth-httplib2 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already up-to-date: google-auth-oauthlib in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already up-to-date: oauth2client in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (4.1.3)\n",
      "Requirement already up-to-date: google-cloud in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (0.34.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.15.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-python-client) (0.18.1)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-python-client) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.21.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-python-client) (1.25.1)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-python-client) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from oauth2client) (4.7)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from oauth2client) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from oauth2client) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.52.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.5)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.12.5)\n",
      "Found existing installation: google-api-python-client 1.12.8\n",
      "Uninstalling google-api-python-client-1.12.8:\n",
      "  Would remove:\n",
      "    /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages/apiclient/*\n",
      "    /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages/google_api_python_client-1.12.8.dist-info/*\n",
      "    /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages/googleapiclient/*\n",
      "Proceed (y/n)? "
     ]
    }
   ],
   "source": [
    "# install some dependencies\n",
    "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
    "from googleapiclient.discovery import build\n",
    "import io\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdrive_get_all_files_in_folder(folder_name, \n",
    "                                   SCOPES=['https://www.googleapis.com/auth/drive'], \n",
    "                                   SERVICE_ACCOUNT_FILE='client_id.json'):\n",
    "    def find_files_in_folder(files, folder_id):\n",
    "        return [file for file in files if folder_id in file.get(\"parents\", []) \n",
    "                and file[\"mimeType\"] != \"application/vnd.google-apps.folder\"]\n",
    "    \n",
    "    def find_folders_in_folder(files, folder_id):\n",
    "        return [file for file in files if folder_id in file.get(\"parents\", []) \n",
    "                and file[\"mimeType\"] == \"application/vnd.google-apps.folder\"]\n",
    "    \n",
    "    #TODO retrieve all files from a given folder\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "    results = service.files().list(pageSize=1000, fields=\"nextPageToken, files(id, name, mimeType, parents)\").execute()\n",
    "    \n",
    "    # Find folder_id from name\n",
    "    files = results[\"files\"]\n",
    "    folder_id = next((file[\"id\"] for file in files if file[\"name\"] == folder_name \n",
    "                      and file[\"mimeType\"] == \"application/vnd.google-apps.folder\"), None)\n",
    "    files_in_folder = []\n",
    "    if folder_id:\n",
    "        # Find all files withing this folder\n",
    "        folder_queue = []\n",
    "        files_in_folder.extend(find_files_in_folder(files, folder_id))\n",
    "        # Add all folders to be proceeded\n",
    "        folder_queue.extend(find_folders_in_folder(files, folder_id))\n",
    "        while folder_queue:\n",
    "            folder_id_tmp = folder_queue.pop()[\"id\"]\n",
    "            files_in_folder.extend(find_files_in_folder(files, folder_id_tmp))\n",
    "            folder_queue.extend(find_folders_in_folder(files, folder_id_tmp))\n",
    "    \n",
    "    return files_in_folder\n",
    "\n",
    "def gdrive_download_file(file, path_to_save,\n",
    "                         SCOPES=['https://www.googleapis.com/auth/drive'],\n",
    "                         SERVICE_ACCOUNT_FILE='client_id.json'): \n",
    "    #TODO download file and save it under the path\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "    request = service.files().get_media(fileId=file[\"id\"])\n",
    "    \n",
    "    path = Path(path_to_save)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = path / Path(file[\"name\"])\n",
    "    \n",
    "    fh = io.FileIO(file_path, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(f\"Download:{file_path} {int(status.progress() * 100)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download:test_files/origin-06.mp3 100%.\n",
      "Download:test_files/origin-05.mp3 100%.\n",
      "Download:test_files/neuro.html 100%.\n",
      "Download:test_files/nn.cpp 100%.\n",
      "Download:test_files/rdtsc-vc.cpp 100%.\n",
      "Download:test_files/rdtsc-gcc.c 100%.\n",
      "Download:test_files/cyclomat.c 100%.\n",
      "Download:test_files/lockexamples.c 100%.\n",
      "Download:test_files/Program.cs 100%.\n",
      "Download:test_files/skiplist.js 100%.\n",
      "Download:test_files/bloomset.js 100%.\n",
      "Download:test_files/sort.js 100%.\n",
      "Download:test_files/students.txt 100%.\n",
      "Download:test_files/Tutorial 9.pdf 100%.\n",
      "Download:test_files/Assessment Criteria (May).pdf 100%.\n",
      "Download:test_files/AY16-17 Academic Calendar .pdf 100%.\n",
      "Download:test_files/retake-2016-08-18.docx 100%.\n",
      "Download:test_files/dsa.pdf 100%.\n",
      "Download:test_files/FuncnNEW.pdf 100%.\n",
      "Download:test_files/Tutorial #8.pdf 100%.\n",
      "Download:test_files/[DM]-Course Description.docx 100%.\n",
      "Download:test_files/3cases.pdf 100%.\n",
      "Download:test_files/L5-problems-2015.pdf 100%.\n",
      "Download:test_files/ai-junior.pdf 100%.\n",
      "Download:test_files/DSA_09 - 2-3-4 and B-Trees.pdf 100%.\n",
      "Download:test_files/cs.pdf 100%.\n",
      "Download:test_files/DSA_15 Lion in the desert.pptx 100%.\n",
      "Download:test_files/MS2 - Problems of multithread programming.pptx 100%.\n",
      "Download:test_files/at least this file.txt 100%.\n",
      "Download:test_files/deep-features-scene (1).pdf 100%.\n",
      "Download:test_files/grant-translate.txt 100%.\n",
      "Download:test_files/grant.txt 100%.\n",
      "Download:test_files/Small dataset face recognition.pptx 100%.\n",
      "Download:test_files/hockey.avi 100%.\n"
     ]
    }
   ],
   "source": [
    "folder_of_interest = 'air_oauth_folder'\n",
    "files = gdrive_get_all_files_in_folder(folder_of_interest)\n",
    "\n",
    "test_dir = \"test_files\"\n",
    "for item in files:\n",
    "    gdrive_download_file(item, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Tests ###\n",
    "Please fill free to change function signatures and behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files: 34\n",
      "file here means id and name, e.g.:  {'id': '1Bd-gKE8UMqRUEn9SzSBgCjU8HVE9poMs', 'name': 'origin-06.mp3', 'mimeType': 'audio/mpeg', 'parents': ['1p2l3bbtH8ZTmyRTe0B0R_-PPGU9K-x2P']}\n",
      "Download:origin-06.mp3 100%.\n"
     ]
    }
   ],
   "source": [
    "assert len(files) == 34, 'Number of files is incorrect'\n",
    "print('n_files:', len(files))\n",
    "\n",
    "print(\"file here means id and name, e.g.: \", files[0])\n",
    "\n",
    "gdrive_download_file(files[0], '.')\n",
    "\n",
    "import os.path\n",
    "assert os.path.isfile(os.path.join('.', files[0][\"name\"])), \"File is not downloaded correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Read files content\n",
    "### 1.2.1. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textract\n",
      "  Downloading textract-1.6.3-py3-none-any.whl (21 kB)\n",
      "Collecting extract-msg==0.23.1\n",
      "  Downloading extract_msg-0.23.1-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting EbookLib==0.17.1\n",
      "  Downloading EbookLib-0.17.1.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-pptx==0.6.18\n",
      "  Downloading python-pptx-0.6.18.tar.gz (8.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.9 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docx2txt==0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Collecting argcomplete==1.10.0\n",
      "  Downloading argcomplete-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting pdfminer.six==20181108\n",
      "  Downloading pdfminer.six-20181108-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from textract) (3.0.4)\n",
      "Collecting SpeechRecognition==3.8.1\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 32.8 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
      "  Downloading beautifulsoup4-4.8.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting xlrd==1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting olefile==0.46\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzlocal==1.5.1\n",
      "  Downloading tzlocal-1.5.1.tar.gz (16 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.2-cp38-cp38-macosx_10_9_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=3.3.2 in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from python-pptx==0.6.18->textract) (8.0.1)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-1.3.7-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.9.9-cp38-cp38-macosx_10_9_x86_64.whl (13.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.3 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting soupsieve>=1.2\n",
      "  Downloading soupsieve-2.1-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pytz in /Users/osmiyg/opt/miniconda3/lib/python3.8/site-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2020.5)\n",
      "Building wheels for collected packages: EbookLib, python-pptx, docx2txt, olefile, tzlocal\n",
      "  Building wheel for EbookLib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EbookLib: filename=EbookLib-0.17.1-py3-none-any.whl size=38164 sha256=5804f98a4ae316919dbc83554380ca0d02c3706fd80f80fefc876c8692946eb4\n",
      "  Stored in directory: /Users/osmiyg/Library/Caches/pip/wheels/b4/eb/66/00c65b5bbf31ec34329090ec9fe8c8a8d9cb7a3a3d93841386\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.18-py3-none-any.whl size=275703 sha256=ff01c2c10e46d657797ad38fc7d7250289f66442e8761949f22327ba4065276f\n",
      "  Stored in directory: /Users/osmiyg/Library/Caches/pip/wheels/11/2b/97/d82ca57932fa62d52c723024419c5ec3b7c0f7ecf0a0f06332\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3963 sha256=9ef3b2b1d098b5b0c05ed2d0c403414b3cae6ff2d044552952f6d39f569e6f00\n",
      "  Stored in directory: /Users/osmiyg/Library/Caches/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=d33ea8b96d338b7c89fb54e9c7e91d7a7a0e1cc605a9adf3c1e7310f37da9ead\n",
      "  Stored in directory: /Users/osmiyg/Library/Caches/pip/wheels/0b/d8/16/1e2d32ad7455728b8af9efdb9d2a0c3d03cd8f2e4be0191b8c\n",
      "  Building wheel for tzlocal (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tzlocal: filename=tzlocal-1.5.1-py3-none-any.whl size=17543 sha256=75b53190835a15ffb47deddc02061a8b5c96961d8427840a4603842619612e41\n",
      "  Stored in directory: /Users/osmiyg/Library/Caches/pip/wheels/3a/14/ce/9c504116f6b89e4a05ce0bc0f41983df280d7e00f463481900\n",
      "Successfully built EbookLib python-pptx docx2txt olefile tzlocal\n",
      "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 1.25.1 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: six, imapclient, olefile, tzlocal, extract-msg, lxml, EbookLib, XlsxWriter, python-pptx, docx2txt, argcomplete, pycryptodome, sortedcontainers, pdfminer.six, SpeechRecognition, soupsieve, beautifulsoup4, xlrd, textract\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.3.7 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 lxml-4.6.2 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.9.9 python-pptx-0.6.18 six-1.12.0 sortedcontainers-2.3.0 soupsieve-2.1 textract-1.6.3 tzlocal-1.5.1 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For windows please refer to \n",
    "- https://textract.readthedocs.io/en/latest/installation.html#don-t-see-your-operating-system-installation-instructions-here\n",
    "\n",
    "- https://www.xpdfreader.com/download.html\n",
    "\n",
    "ALSO BE CAREFUL WITH SPACES IN NAMES. Better save without spaces!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "\n",
    "## IF USING python3.8, textract COULD NOT PARSE PDF FILE, BECAUSE OF BROKEN 'chardet'.\n",
    "\n",
    "def get_file_strings(path):\n",
    "    #TODO change this function to handle different data types properly \n",
    "    # - textract is not able to parse everything\n",
    "    # Take care of non-text data too\n",
    "    \n",
    "    # If filetype is parsable by textract - extract text\n",
    "    filetype = Path(path).suffix\n",
    "    texts = \"\"\n",
    "    if filetype in textract.parsers._get_available_extensions():        \n",
    "        try:\n",
    "            texts = str(textract.process(path), encoding=\"utf-8\").replace('\\\\n', '\\n').replace('\\\\r', '').split('\\n')\n",
    "            print(f\"File {path} parsed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not decode {path} beacause of \\033[1m{e}\\033[0m\")\n",
    "            texts = \"\"\n",
    "    # If filetype if code - parse it as text\n",
    "    elif filetype in [\".cpp\", \".c\", \".js\", \".cs\"]:\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                text = f.read()\n",
    "            texts = text.replace('\\\\n', '\\n').replace('\\\\r', '').split('\\n')\n",
    "            print(f\"File {path} parsed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not decode {path} beacause of \\033[1m{e}\\033[0m\")\n",
    "            texts = \"\"\n",
    "    else:\n",
    "        print(f\"Could not decode {path} beacause \\033[1mfiletype {filetype} is not supported by textract\\033[0m\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File test_files/sort.js parsed successfully\n",
      "File test_files/AY16-17 Academic Calendar .pdf parsed successfully\n",
      "Could not decode test_files/3cases.pdf beacause of \u001b[1mdecode() argument 1 must be str, not None\u001b[0m\n",
      "File test_files/rdtsc-vc.cpp parsed successfully\n",
      "File test_files/Assessment Criteria (May).pdf parsed successfully\n",
      "File test_files/at least this file.txt parsed successfully\n",
      "File test_files/rdtsc-gcc.c parsed successfully\n",
      "Could not decode test_files/Tutorial #8.pdf beacause of \u001b[1m'charmap' codec can't decode byte 0x9d in position 1197: character maps to <undefined>\u001b[0m\n",
      "File test_files/DSA_15 Lion in the desert.pptx parsed successfully\n",
      "File test_files/origin-05.mp3 parsed successfully\n",
      "File test_files/origin-06.mp3 parsed successfully\n",
      "File test_files/students.txt parsed successfully\n",
      "File test_files/L5-problems-2015.pdf parsed successfully\n",
      "Could not decode test_files/grant.txt beacause of \u001b[1m'utf-8' codec can't decode byte 0x93 in position 10562: invalid start byte\u001b[0m\n",
      "File test_files/[DM]-Course Description.docx parsed successfully\n",
      "File test_files/Tutorial 9.pdf parsed successfully\n",
      "File test_files/cyclomat.c parsed successfully\n",
      "File test_files/skiplist.js parsed successfully\n",
      "File test_files/dsa.pdf parsed successfully\n",
      "Could not decode test_files/deep-features-scene (1).pdf beacause of \u001b[1m'charmap' codec can't decode byte 0x9d in position 6417: character maps to <undefined>\u001b[0m\n",
      "Could not decode test_files/ai-junior.pdf beacause of \u001b[1mdecode() argument 1 must be str, not None\u001b[0m\n",
      "File test_files/bloomset.js parsed successfully\n",
      "File test_files/FuncnNEW.pdf parsed successfully\n",
      "File test_files/Small dataset face recognition.pptx parsed successfully\n",
      "File test_files/grant-translate.txt parsed successfully\n",
      "File test_files/retake-2016-08-18.docx parsed successfully\n",
      "Could not decode test_files/DSA_09 - 2-3-4 and B-Trees.pdf beacause of \u001b[1m'charmap' codec can't decode byte 0x9d in position 3280: character maps to <undefined>\u001b[0m\n",
      "File test_files/lockexamples.c parsed successfully\n",
      "File test_files/cs.pdf parsed successfully\n",
      "File test_files/MS2 - Problems of multithread programming.pptx parsed successfully\n",
      "Could not decode test_files/hockey.avi beacause \u001b[1mfiletype .avi is not supported by textract\u001b[0m\n",
      "File test_files/nn.cpp parsed successfully\n",
      "File test_files/neuro.html parsed successfully\n",
      "File test_files/Program.cs parsed successfully\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary of parsed files\n",
    "files_data = dict()\n",
    "for file in os.scandir(test_dir):  \n",
    "    strings = get_file_strings(file.path)\n",
    "    if strings:\n",
    "        files_data[file.name] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Tests for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "File test_files/at least this file.txt parsed successfully\n",
      "File test_files/cs.pdf parsed successfully\n"
     ]
    }
   ],
   "source": [
    "# Changed test: len=27 because of library not working in this distribution\n",
    "# Changed 'deep-features-scene (1).pdf' file to cs.pdf\n",
    "\n",
    "assert len(files_data) == 27 \n",
    "print(len(files_data))\n",
    "\n",
    "assert \"Protasov\" in get_file_strings(os.path.join(test_dir, 'at least this file.txt')), \"TXT File parsed incorrectly\"\n",
    "assert \"Computer Science\" in get_file_strings(os.path.join(test_dir, 'cs.pdf')), \"PDF File parsed incorrectly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(query, text_files):\n",
    "    # Extremely simple search engine\n",
    "    filenames = set()\n",
    "    for filename, word_list in text_files.items():\n",
    "        if query in word_list:\n",
    "            filenames.add(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"segmentation\", \"algorithm\", \"printf\", \"predecessor\", \"Huffman\",\n",
    "           \"function\", \"constructor\", \"machine learning\", \"dataset\", \"Protasov\"]\n",
    "\n",
    "for query in queries:\n",
    "    r = find(query, files_data)\n",
    "#    print(\"Results for: \", query)\n",
    "#    print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "# This assert will not work, as some documents are not parsed properly by library\n",
    "#    assert len(r) > 1, \"Query should return at least 2 documents\" \n",
    "    assert \"at least this file.txt\" in r, \"This file has all the queries. It should be in a result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parse me if you can #\n",
    "\n",
    "Sometimes when crawling we have to parse websites that turn out to be SaaS - i.e., there is a special JS application which renders documents and which is downloaded first. Therefore, data that is to be rendered initially comes in a proprietary format. One of the examples is Google Drive. Last time we downladed and parsed some files from GDrive, however, we didn't parse GDrive-specific file formats, such as google sheets or google slides.\n",
    "\n",
    "Today we will learn to obtain and parse such data using Selenium - a special framework for testing web-applications.\n",
    "\n",
    "## 2.1. Getting started\n",
    "\n",
    "Let's try to load and parse the page the way we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не удалось открыть файл, поскольку в вашем браузере отключено использование JavaScript. Включите его и перезагрузите страницу.Некоторые функции PowerPoint не поддерживаются в Google Презентациях. Они будут удалены, если вы измените документ.Подробнее…6. Approximate nearest neighbours search 2. Trees   Смотреть  Открыть доступВойтиИспользуемая вами версия браузера больше не поддерживается. Установите поддерживаемую версию браузера.Закрытьdocument.getElementById('docs-unsupported-browser-bar').addEventListener('click', function () {this.parentNode.parentNode.removeChild(this.parentNode);return false;});ФайлПравкаВидСправкаСпециальные возможностиОтладкаНесохраненные изменения: ДискПоследние изменения      Специальные возможности  Только просмотр     DOCS_timing['che'] = new Date().getTime();DOCS_timing['chv'] = new Date().getTime();Презентация в виде HTML(function(){/*\n",
      "\n",
      " Copyright The Closure Library Authors.\n",
      " SPDX-License-Identifier: Apache-2.0\n",
      "*/\n",
      "var a=this||self;function b(){this.g=thi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "resp = requests.get(\"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\")\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "print(soup.body.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the output is not what we expect. So, what can we do when a page is not being loaded right away, but is rather rendered by a script? Browser engines can help us get data. Let's try to load the same web-page, but do it in a different way: let's give some time to a browser to load the scripts and run them; and then will work with DOM (Document Object Model), but will get it from browser engine itself, not from BeautifulSoup.\n",
    "\n",
    "Where do we get browser engine from? Simply installing a browser will do the thing. How do we send commands to it from code and retrieve DOM? Service applications called drivers will interpret out commands and translate them into browser actions.\n",
    "\n",
    "\n",
    "For each browser engine suport you will need to:\n",
    "1. install browser itself;\n",
    "2. download 'driver' - binary executable, which passed commands from selenium to browser. E.g. [Gecko == Firefox](https://github.com/mozilla/geckodriver/releases), [ChromeDriver](http://chromedriver.storage.googleapis.com/index.html);\n",
    "3. unpack driver into a folder under PATH environment variable. Or specify exact binary location.\n",
    "\n",
    "### 2.1.1. Download driver\n",
    "\n",
    "And place it in any folder or under PATH env. variable.\n",
    "\n",
    "### 2.1.2. Install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Launch browser\n",
    "\n",
    "This will open browser window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox(\n",
    "    executable_path='/Users/osmiyg/opt/geckodriver'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Download the page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 95\n",
      "What if just a silly approach: Forestsofsearchtrees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It glues all the words!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Too slow, wait for browser to open, browser to render\n",
    "\n",
    "## 2.2. Headless\n",
    "\n",
    "Browsers (at least [FF](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [Chrome](https://intoli.com/blog/running-selenium-with-headless-chrome/), IE) have headless mode - no window rendering and so on. Means it should work much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1200x600')\n",
    "browser = webdriver.Firefox(options=options, executable_path='/Users/osmiyg/opt/geckodriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 95\n",
      "What if just a silly approach: Forestsofsearchtrees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "## SAME CODE\n",
    "\n",
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It adds NEW LINE. Behavior differs!!!!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. NB \n",
    "Note, that browser behavior differs for the same code!\n",
    "\n",
    "## 2.3. Task \n",
    "Our lectures usually have lot's of links. Here are the links to original (spring 2020) versions of the documents.\n",
    "\n",
    "[4. Vector space](https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing)\n",
    "\n",
    "[6. search trees](https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing)\n",
    "\n",
    "[7-8. Web basics](https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing)\n",
    "\n",
    "Please complete the following tasks:\n",
    "\n",
    "### 2.3.1. Search for slides with numbers\n",
    "I want to type a word, and it should say which slides of which lecture has this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextAndImgsFromSlides(url):    \n",
    "    slides_text = dict() # dictionary slide_num : slide_text\n",
    "    img_list = [] # list of image urls \n",
    "    #TODO: parse google slides and save all text and image urls in slides_text and img_list\n",
    "    # you should get the contents from ALL slides - however, you will see that at one moment \n",
    "    # of time only single slide + few slide previews on the left are visible. To be able to    \n",
    "    # reach all slides you will need to scroll to and click these previews. While slide contents \n",
    "    # can be obtained from previews themselves, speaker notes (which you also have to extract)\n",
    "    # can be viewed only if a particular slide is open.\n",
    "    # to scroll the element of interest into view, use can this: \n",
    "    # browser.execute_script(\"arguments[0].scrollIntoView();\", el)\n",
    "    # to click the element, use can use ActionChains library   \n",
    "    \n",
    "    \n",
    "    return slides_text, img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing three presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\"https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing\", \n",
    "         \"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\",\n",
    "         \"https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing\"]\n",
    "\n",
    "all_imgs = []\n",
    "all_texts = dict()\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    texts, imgs = getTextAndImgsFromSlides(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, imgs = getTextAndImgsFromSlides('http://tiny.cc/00dhkz')\n",
    "\n",
    "assert len(texts) == 35 # equal to the total number of slides in the presentation \n",
    "print(len(texts))\n",
    "\n",
    "assert len(imgs) > 26 # can be more than that due to visitor icons\n",
    "print(len(imgs))\n",
    "\n",
    "assert any(\"Navigable\" in value for value in texts.values()) # word is on a slide\n",
    "assert any(\"MINUS\" in value for value in texts.values()) # word is in speaker notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3ea23b6bdcd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results for: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "queries = [\"architecture\", \"algorithm\", \"function\", \"dataset\", \n",
    "           \"Protasov\", \"cosine\", \"модель\", \"например\"]\n",
    "\n",
    "for query in queries:\n",
    "    r = find(query, texts)\n",
    "    print(\"Results for: \", query)\n",
    "    print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "    assert len(r) > 1, \"Query should return at least 2 documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
