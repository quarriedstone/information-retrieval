{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Building inverted index and answering queries\n",
    "\n",
    "In this lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it: starting from crawling documents, then building an inverted index, answering queries using this index, and organizing it as a simple web server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a unified approach to documents preprocessing, and this class is responsible for it. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                      'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #TODO word tokenize text using nltk lib\n",
    "        return ['one', 'two', 'three']\n",
    "\n",
    "    \n",
    "    def stem(self, word, stemmer):\n",
    "        #TODO stem word using provided stemmer\n",
    "        return 'stemmed_word'\n",
    "\n",
    "    \n",
    "    def is_apt_word(self, word):\n",
    "        #TODO check if word is appropriate - not a stop word and isalpha, \n",
    "        # i.e consists of letters, not punctuation, numbers, dates\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        #TODO combine all previous methods together: tokenize lowercased text \n",
    "        # and stem it, ignoring not appropriate words\n",
    "        return ['one', 'two', 'three']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Crawling and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Base classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some base classes we will need for writing our indexer. The code from the last lab's solution is given, but note that you will need to change some of it, namely, the `parse` function. The reason is it always makes complete parsing, which we want to avoid when we only need links, for example, or a specific portion of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        with open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO change this method\n",
    "        \n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        model = BeautifulSoup(self.content)\n",
    "        \n",
    "        self.anchors = []\n",
    "        a = model.find_all('a')\n",
    "        for anchor in a:\n",
    "            href = self.normalize(anchor.get('href'))\n",
    "            text = anchor.text\n",
    "            self.anchors.append((text, href))\n",
    "                        \n",
    "        self.images = []\n",
    "        i = model.find_all('img')\n",
    "        for img in i:\n",
    "            href = self.normalize(img.get('src'))\n",
    "            self.images.append(href)\n",
    "        \n",
    "        texts = model.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)  \n",
    "        self.text = u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Main class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (of a form https://www.reuters.com/...) for visiting. To speed up, doesn't consider for visiting pages with content type other than html: '.pdf', '.mp3', '.avi', '.mp4', '.txt'. If encounters an article page (of a form https://www.reuters.com/article/...), saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`, `doc_id:url`\n",
    "    - `index`, `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    - `doc_lengths`, `doc_id:doc_length` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "\n",
    "**Bonus task \\*** In real industrial systems a crawler would pass the links to the dedicated service that would load their contents in a bunch of parallel threads. Implement such a service - get urls as inputs, load page contents in parallel and return filenames on disk, which are then processed by indexer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):      \n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}        \n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "        \n",
    "    \n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):        \n",
    "        #TODO generate url-s for visiting\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def index_doc(self, doc, doc_id):\n",
    "        #TODO add documents to index\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.reuters.com/news/us\n",
      "2 https://www.reuters.com/\n",
      "3 https://www.reuters.com/finance\n",
      "4 https://www.reuters.com/finance/markets\n",
      "5 https://www.reuters.com/news/world\n",
      "6 https://www.reuters.com/politics\n",
      "7 https://www.reuters.com/video\n",
      "8 https://www.reuters.com/news/archive/domesticNews\n",
      "9 https://www.reuters.com/article/us-usa-blast/blast-at-machine-shop-rips-through-houston-neighborhood-killing-two-idUSKBN1ZN18J\n",
      "10 https://www.reuters.com/article/us-usa-mexico-corruption/u-s-charges-former-mexico-police-commander-in-el-chapo-linked-cocaine-probe-idUSKBN1ZN206\n",
      "11 https://www.reuters.com/news/archive/politicsNews\n",
      "12 https://www.reuters.com/article/us-usa-trump-impeachment/democrats-in-impeachment-trial-say-trump-abused-his-power-for-political-gain-idUSKBN1ZM1Q7\n",
      "13 https://www.reuters.com/news/archive/technologyNews\n",
      "14 https://www.reuters.com/article/us-internet-regulation-justice/u-s-justice-department-plans-to-hold-meeting-to-discuss-tech-industry-liability-sources-idUSKBN1ZN2E5\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/technology\", 2, \"test_collection\", 5):\n",
    "    print(k, c.url)\n",
    "    k+=1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Building index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/\", 3, \"docs_collection\"):\n",
    "    print(k, c.url)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Index statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load index, doc_lengths and doc_urls\n",
    "with open('inverted_index.p', 'rb') as fp:\n",
    "    index = pickle.load(fp)\n",
    "with open('doc_lengths.p', 'rb') as fp:\n",
    "    doc_lengths = pickle.load(fp)\n",
    "with open('doc_urls.p', 'rb') as fp:\n",
    "    doc_urls = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index length 14247\n",
      "\n",
      "Top terms by number of documents they apperared in:\n",
      "[('reuter', 717), ('edit', 675), ('s', 670), ('said', 660), ('report', 649), ('have', 509), ('year', 477), ('not', 471), ('which', 466), ('but', 465), ('thi', 461), ('new', 429), ('after', 423), ('more', 421), ('their', 383), ('had', 381), ('over', 380), ('file', 379), ('last', 376), ('who', 373)]\n",
      "\n",
      "Top terms by overall frequency:\n",
      "[('s', 3695), ('said', 3121), ('have', 1454), ('year', 1293), ('not', 1186), ('reuter', 1149), ('but', 1112), ('report', 1092), ('new', 1073), ('hi', 1046), ('thi', 970), ('which', 949), ('they', 944), ('who', 906), ('more', 876), ('after', 866), ('we', 839), ('their', 835), ('had', 767), ('compani', 758)]\n"
     ]
    }
   ],
   "source": [
    "print('Total index length', len(index))\n",
    "print('\\nTop terms by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "print('\\nTop terms by overall frequency:')\n",
    "sorted_by_freq = sorted(index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Answering query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        #TODO retrieve a set of documents containing all query terms\n",
    "        return {0, 1, 3}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        #TODO retrieve relevant documents with scores\n",
    "        return {0: 0.32, 5: 1.17}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4:20, 5:30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4,2)]}\n",
    "\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1,2,3))\n",
    "assert not any(k in okapi_res for k in (4,5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setting up a server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus task \\*** Organize the resulting search engine as a web-service that gets a query from get-parameters and returns urls with scores as a `json` dictionary. Check its work in a browser of with curl, should look smth like this:\n",
    " \n",
    "`> curl localhost:8080/?q=some_query_text\n",
    "{ \"url1\" : 0.9, \"url2\": 0.8 }`\n",
    "\n",
    "You can use one of the following tools for this task: https://www.acmesystems.it/python_http, http.server.ThreadingHTTPServer (3.7+) https://docs.python.org/3/library/http.server.html#http.server.SimpleHTTPRequestHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO write a web-service that answers queries using inverted index\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
